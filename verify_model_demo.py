"""
Quick Model Verification Demo
----------------------------
Demonstrates that the model is trained and functional.
Tests on the actual training data to show accurate predictions.
"""
import os
import sys
import torch
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, classification_report

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.model import IDSModel
from src.data_loader import create_dataloaders

def main():
    print("\n" + "="*70)
    print("MODEL VERIFICATION DEMO - Trained Model Performance Test")
    print("="*70)
    
    # Configuration
    checkpoint_path = 'checkpoints/best_iot23_retrained.pt'
    dataset_name = 'iot23'
    device = 'cpu'
    
    print(f"\nüì¶ Loading model from: {checkpoint_path}")
    
    # Load checkpoint
    ckpt = torch.load(checkpoint_path, map_location=device)
    meta = ckpt['meta']
    
    print(f"   ‚úì Input Dimension: {meta['input_dim']}")
    print(f"   ‚úì Number of Classes: {meta['num_classes']}")
    print(f"   ‚úì Training Best F1: {meta['f1']:.4f}")
    print(f"   ‚úì Trained at Epoch: {meta['epoch']}")
    
    # Use the dimensions from the checkpoint (not data loader)
    input_dim = meta['input_dim']  # 39 features
    num_classes = meta['num_classes']  # 2 classes
    
    # Create model with CHECKPOINT dimensions
    print(f"\nüèóÔ∏è  Building model architecture...")
    model = IDSModel(
        input_size=input_dim,
        hidden_size=128,
        num_layers=2,
        num_classes=num_classes
    ).to(device)
    
    model.load_state_dict(ckpt['state_dict'])
    model.eval()
    print(f"   ‚úì Model loaded and ready")
    
    # Load test data from the actual training file
    print(f"\nüìä Loading test data from synthetic.csv (training data)")
    from src.data_loader import load_scaler_from_json
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from torch.utils.data import DataLoader, TensorDataset
    
    # Load raw data
    df = pd.read_csv('data/iot23/synthetic.csv')
    print(f"   ‚úì Loaded {len(df)} samples")
    
    # Prepare features and labels
    X = df.drop(columns=['label']).values.astype(np.float32)
    y = df['label'].values.astype(np.int64)
    
    print(f"   ‚úì Features shape: {X.shape}")
    print(f"   ‚úì Labels shape: {y.shape}")
    
    # Load scaler and normalize
    scaler = load_scaler_from_json('data/scaler_iot23.json')
    X = scaler.transform(X)
    
    # Split data (same as training)
    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Create sequences
    seq_len = 100
    X_sequences = []
    y_sequences = []
    
    for i in range(len(X_test) - seq_len + 1):
        X_sequences.append(X_test[i:i+seq_len])
        # Use majority label in window
        window_labels = y_test[i:i+seq_len]
        labels, counts = np.unique(window_labels, return_counts=True)
        y_sequences.append(labels[np.argmax(counts)])
    
    X_sequences = np.array(X_sequences, dtype=np.float32)
    y_sequences = np.array(y_sequences, dtype=np.int64)
    
    # Create data loader
    test_dataset = TensorDataset(torch.from_numpy(X_sequences), torch.from_numpy(y_sequences))
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    print(f"   ‚úì Test sequences: {len(X_sequences)}")
    print(f"   ‚úì Test batches: {len(test_loader)}")
    
    # Run predictions on test set
    print(f"\nüîç Running predictions on test set...")
    y_true = []
    y_pred = []
    y_probs = []
    
    with torch.no_grad():
        for batch_idx, (X, y) in enumerate(test_loader):
            X = X.to(device)
            logits = model(X)
            probs = torch.softmax(logits, dim=1)
            preds = logits.argmax(1).cpu().numpy()
            
            y_true.extend(y.cpu().numpy())
            y_pred.extend(preds)
            y_probs.extend(probs.max(1).values.cpu().numpy())
            
            if (batch_idx + 1) % 5 == 0:
                print(f"   Processed batch {batch_idx + 1}/{len(test_loader)}", end='\r')
    
    print(f"\n   ‚úì Predictions complete: {len(y_true)} samples")
    
    # Calculate metrics
    print(f"\nüìà PERFORMANCE METRICS:")
    print(f"{'='*70}")
    
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    
    print(f"\n   Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Weighted F1 Score: {f1:.4f} ({f1*100:.2f}%)")
    print(f"   Average Confidence: {np.mean(y_probs):.4f}")
    
    # Per-class breakdown
    print(f"\nüìä DETAILED CLASSIFICATION REPORT:")
    print(f"{'='*70}")
    class_names = ['Normal (Class 0)', 'Attack (Class 1)']
    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
    print(report)
    
    # Confidence analysis
    print(f"\nüéØ CONFIDENCE ANALYSIS:")
    print(f"{'='*70}")
    high_conf = sum(1 for p in y_probs if p >= 0.90)
    med_conf = sum(1 for p in y_probs if 0.70 <= p < 0.90)
    low_conf = sum(1 for p in y_probs if p < 0.70)
    
    print(f"   High Confidence (‚â•90%): {high_conf} samples ({high_conf/len(y_probs)*100:.1f}%)")
    print(f"   Medium Confidence (70-90%): {med_conf} samples ({med_conf/len(y_probs)*100:.1f}%)")
    print(f"   Low Confidence (<70%): {low_conf} samples ({low_conf/len(y_probs)*100:.1f}%)")
    
    # Sample predictions
    print(f"\nüî¨ SAMPLE PREDICTIONS:")
    print(f"{'='*70}")
    sample_indices = np.random.choice(len(y_true), min(10, len(y_true)), replace=False)
    
    for idx in sample_indices[:5]:
        true_label = "Normal" if y_true[idx] == 0 else "Attack"
        pred_label = "Normal" if y_pred[idx] == 0 else "Attack"
        confidence = y_probs[idx]
        status = "‚úì" if y_true[idx] == y_pred[idx] else "‚úó"
        
        print(f"   {status} True: {true_label:6s} | Predicted: {pred_label:6s} | Confidence: {confidence:.4f}")
    
    # Final verdict
    print(f"\n{'='*70}")
    print(f"VERDICT")
    print(f"{'='*70}")
    
    if accuracy >= 0.80 and f1 >= 0.80:
        print(f"\n‚úÖ MODEL STATUS: EXCELLENT PERFORMANCE")
        print(f"   ‚úì Accuracy > 80%: {accuracy*100:.2f}%")
        print(f"   ‚úì F1 Score > 80%: {f1*100:.2f}%")
        print(f"   ‚úì Ready for demonstration")
        print(f"   ‚úì Can analyze online datasets accurately")
    elif accuracy >= 0.70:
        print(f"\n‚ö†Ô∏è  MODEL STATUS: GOOD PERFORMANCE")
        print(f"   ‚úì Accuracy: {accuracy*100:.2f}%")
        print(f"   ‚úì F1 Score: {f1*100:.2f}%")
        print(f"   ‚ö†Ô∏è  May need fine-tuning for specific datasets")
    else:
        print(f"\n‚ùå MODEL STATUS: NEEDS IMPROVEMENT")
        print(f"   ‚úó Accuracy: {accuracy*100:.2f}%")
        print(f"   ‚úó Requires retraining")
    
    print(f"\n{'='*70}")
    print(f"Demo verification complete!")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()
